{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.io\n",
    "import graph\n",
    "import task\n",
    "import func\n",
    "from algorithms import dissim, netmf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare random states for train-test-split in node classification (for reproduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "rounds = 10\n",
    "sub_rounds = 10\n",
    "file_rs = 'node_classification_rs.npy'\n",
    "rs_mat = np.random.randint(low=1, high=1000, size=(rounds, sub_rounds), dtype=int)\n",
    "np.save(file_rs, rs_mat)\n",
    "\n",
    "# read\n",
    "# file_rs = 'node_classification_rs.npy'  \n",
    "# rs_mat = np.load(file_rs)               \n",
    "# print(rs_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare node labels for node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a list of strings to txt\n",
    "def save_list_str(data, file):\n",
    "    with open(file, 'w') as fp:\n",
    "        for item in data:\n",
    "            fp.write('%s\\n' % item)\n",
    "    return\n",
    "\n",
    "# read a list of strings from txt\n",
    "def read_list_str(file):\n",
    "    data = []\n",
    "    with open(file, 'r') as fp:\n",
    "        for line in fp:\n",
    "            data.append(line[:-1])\n",
    "    return data\n",
    "\n",
    "graph_name = 'PPI'\n",
    "node_attr = 'label'\n",
    "file_labels = 'labels/' + graph_name + '_labels.txt' \n",
    "file_Y = 'labels/' + graph_name + '_Y.npy'\n",
    "\n",
    "# save data\n",
    "nx_G = graph.gen_graph(graph_name)\n",
    "labels, n_labels = graph.find_labels(nx_G, node_attr)\n",
    "save_list_str(labels, file_labels)\n",
    "n_samples = len(nx_G)\n",
    "n_labels = len(labels)\n",
    "Y = np.zeros((n_samples, n_labels), dtype=int)\n",
    "ns = 0\n",
    "for node in nx_G.nodes(data=node_attr):\n",
    "    for label in node[1]:\n",
    "        Y[ns, labels.index(label)] = 1 \n",
    "    ns += 1\n",
    "np.save(file_Y, Y)\n",
    "\n",
    "# load data\n",
    "# labels = read_list_str(file_labels)\n",
    "# Y = np.load(file_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for single-label datasets (Citeseer and Cora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_name = 'Cora'\n",
    "node_attr = 'label'\n",
    "\n",
    "file_Y = 'labels/' + graph_name + '_TrueLabels.npy'\n",
    "\n",
    "if graph_name == 'Citeseer':\n",
    "    label_dict = {'DB': 0, 'Agents': 1, 'ML': 2, 'HCI': 3, 'IR': 4, 'AI': 5}\n",
    "else: # Core\n",
    "    label_dict = {'Probabilistic_Methods': 0, 'Case_Based': 1, 'Neural_Networks': 2, 'Theory': 3, \n",
    "              'Reinforcement_Learning': 4, 'Rule_Learning': 5, 'Genetic_Algorithms': 6}\n",
    "\n",
    "# save data\n",
    "nx_G = graph.gen_graph(graph_name)\n",
    "n_samples = len(nx_G)\n",
    "Y = np.zeros(n_samples, dtype=int)\n",
    "ns = 0\n",
    "for node in nx_G.nodes(data=node_attr):\n",
    "    assert len(node[1]) == 1  # single label\n",
    "    Y[ns] = label_dict[node[1][0]]\n",
    "    ns += 1\n",
    "np.save(file_Y, Y)\n",
    "\n",
    "# load data\n",
    "# Y = np.load(file_Y)\n",
    "# print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare matrices (node classification)\n",
    "#### HOPE: prepare adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_name = 'BlogCatalog'\n",
    "nx_G = graph.gen_graph(graph_name)\n",
    "A = np.array(nx.to_numpy_matrix(nx_G, weight=None)) # consider undirected graphs here\n",
    "file = 'mats/Katz_nc/unweighted_adj/' + graph_name + '.mat'\n",
    "scipy.io.savemat(file, mdict={'adj': A})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Katz (might not be same as the result given by Matlab for large graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_name = 'BlogCatalog'\n",
    "ratio = 0.95\n",
    "nx_G = graph.gen_graph(graph_name)\n",
    "graph_size = nx_G.number_of_nodes()\n",
    "A = np.array(nx.to_numpy_matrix(nx_G, weight=None))\n",
    "beta_dict = {'Citeseer': 0.0691, 'Cora': 0.0660, 'PPI': 0.0126,'BlogCatalog': 0.0029}     \n",
    "beta = beta_dict[graph_name]\n",
    "S = np.matmul(np.linalg.inv(np.eye(graph_size)-beta*A), beta*A)\n",
    "file = 'mats/Katz_nc/katz/' + graph_name + '_' + str(ratio) + '.npy'\n",
    "np.save(file, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare deepwalk based similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_name = 'Cora'\n",
    "window = 10\n",
    "b = 1\n",
    "nx_G = graph.gen_graph(graph_name)\n",
    "A = nx.to_scipy_sparse_matrix(nx_G, weight='weight')\n",
    "S = netmf.comp_S_DW(A, window, b) # numpy matrix\n",
    "file = 'mats/DW_nc/'+graph_name+'_T'+str(window)+'_b'+str(b)+'.npy'\n",
    "np.save(file, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare FE based similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inf = 1e8\n",
    "# graph_name = 'Cora'\n",
    "# beta_list = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1]\n",
    "\n",
    "# nx_G = graph.gen_graph(graph_name)\n",
    "# A = np.array(nx.to_numpy_matrix(nx_G, weight='weight'))\n",
    "# d = np.sum(A, 0)\n",
    "# C = dissim.adj2cost(A, inf)\n",
    "# graph_size = len(nx_G)\n",
    "# H = np.eye(graph_size) - np.ones((graph_size, graph_size))/graph_size\n",
    "\n",
    "# for beta in beta_list:\n",
    "#     print(beta)\n",
    "#     #start = time.time()\n",
    "#     Delta_FE = dissim.FE(A, C, d, beta)\n",
    "#     #end = time.time()\n",
    "#     #print(end-start)\n",
    "#     K = -0.5*np.matmul(np.matmul(H,Delta_FE),H) \n",
    "#     file_fe = 'mats/FE_nc/'+graph_name+'_'+str(beta)+'.npy'\n",
    "#     np.save(file_fe, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = 1e8\n",
    "graph_name = 'Citeseer'\n",
    "# beta_list = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1]\n",
    "beta_list = [20]\n",
    "\n",
    "nx_G = graph.gen_graph(graph_name)\n",
    "A = np.array(nx.to_numpy_matrix(nx_G, weight='weight'))\n",
    "d = np.sum(A, 0)\n",
    "C = dissim.adj2cost(A, inf)\n",
    "\n",
    "for beta in beta_list:\n",
    "    print(beta)\n",
    "    Delta_FE = dissim.FE(A, C, d, beta)\n",
    "    file_fe = 'mats_v2/FE_nc/'+graph_name+'_'+str(beta)+'.npy'\n",
    "    np.save(file_fe, Delta_FE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare data for link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_name = 'Citeseer'      \n",
    "test_ratio = 0.3\n",
    "rs = [965, 177, 218, 342, 383, 254, 108, 37, 760, 404]\n",
    "rounds = len(rs) # 10\n",
    "dir_preprocessed = 'LP_data/' + graph_name + '_' + str(test_ratio) + '/' # need to create in advance\n",
    "nx_G = graph.gen_graph(graph_name)\n",
    "for round_i in range(rounds):\n",
    "    train_graph, train_edges, train_labels, test_edges, test_labels = task.lp_split_train_test(nx_G, test_ratio, rs[round_i])\n",
    "    print(round_i, train_graph.number_of_nodes(), len(train_edges), len(test_edges))\n",
    "    f_train_graph  = dir_preprocessed + 'train_graph_'  + str(round_i) + '.gpickle'\n",
    "    f_train_edges  = dir_preprocessed + 'train_edges_'  + str(round_i) + '.txt'\n",
    "    f_train_labels = dir_preprocessed + 'train_labels_' + str(round_i) + '.txt'\n",
    "    f_test_edges   = dir_preprocessed + 'test_edges_'   + str(round_i) + '.txt'\n",
    "    f_test_labels  = dir_preprocessed + 'test_labels_'  + str(round_i) + '.txt'\n",
    "    nx.write_gpickle(train_graph, f_train_graph)\n",
    "    func.save_list(train_edges, f_train_edges)\n",
    "    func.save_list(train_labels, f_train_labels)\n",
    "    func.save_list(test_edges, f_test_edges)\n",
    "    func.save_list(test_labels, f_test_labels)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the data\n",
    "# nx_G_edges = list(map(set,list(nx_G.edges())))\n",
    "# train_graph_edges = list(map(set,list(train_graph.edges())))\n",
    "\n",
    "# ns = 0\n",
    "# for edge in train_edges:\n",
    "#     if train_labels[ns] == 1:\n",
    "#         assert set(edge) in train_graph_edges\n",
    "#     else:\n",
    "#         assert set(edge) not in nx_G_edges\n",
    "#     ns += 1    \n",
    "\n",
    "# ns = 0\n",
    "# for edge in test_edges:\n",
    "#     if test_labels[ns] == 1:\n",
    "#         assert set(edge) not in train_graph_edges\n",
    "#         assert set(edge) in nx_G_edges\n",
    "#     else:\n",
    "#         assert set(edge) not in nx_G_edges\n",
    "#     ns += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare matrices (link prediction)\n",
    "#### HOPE: prepare adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_name = 'BlogCatalog'      \n",
    "test_ratio = 0.2\n",
    "rounds = 10\n",
    "dir_preprocessed = 'LP_data/' + graph_name + '_' + str(test_ratio) + '/'\n",
    "for round_i in range(rounds):\n",
    "    print(round_i)\n",
    "    f_train_graph  = dir_preprocessed + 'train_graph_'  + str(round_i) + '.gpickle'\n",
    "    train_graph  = func.load_list(f_train_graph)\n",
    "    A = np.array(nx.to_numpy_matrix(train_graph, weight=None))\n",
    "    file = 'mats/Katz_lp_' + str(test_ratio) + '/unweighted_adj/' + graph_name + '_' + str(round_i) + '.mat'\n",
    "    scipy.io.savemat(file, mdict={'adj': A})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Katz (to be changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_name = 'BlogCatalog'\n",
    "test_ratio = 0.5\n",
    "rounds = 1\n",
    "dir_preprocessed = 'LP_data/' + graph_name + '_' + str(test_ratio) + '/'\n",
    "ratio = 0.95\n",
    "beta_dict = {'Citeseer': [0.1229, 0.1122, 0.1133, 0.1110, 0.1176, 0.1162, 0.1215, 0.1120, 0.1095, 0.1202], \n",
    "             'Cora': [0.1049, 0.0967, 0.0955, 0.1044, 0.0964, 0.1015, 0.0977, 0.0922, 0.1017, 0.1018], \n",
    "             'PPI': [0.0246, 0.0248, 0.0241, 0.0246, 0.0243, 0.0246, 0.0247, 0.0248, 0.0249, 0.0245],\n",
    "             'BlogCatalog': [0.0058, 0.0058, 0.0058, 0.0058, 0.0058, 0.0058, 0.0058, 0.0058, 0.0058, 0.0058]}   \n",
    "for round_i in range(rounds):\n",
    "    print(round_i)\n",
    "    f_train_graph  = dir_preprocessed + 'train_graph_'  + str(round_i) + '.gpickle'\n",
    "    nx_G  = func.load_list(f_train_graph)\n",
    "    graph_size = nx_G.number_of_nodes()\n",
    "    A = np.array(nx.to_numpy_matrix(nx_G, weight=None))\n",
    "    beta = beta_dict[graph_name][round_i]\n",
    "    S = np.matmul(np.linalg.inv(np.eye(graph_size)-beta*A), beta*A)\n",
    "    file = 'mats/Katz_lp/katz/' + graph_name + '_' + str(ratio) + '_' + str(round_i) + '.npy'\n",
    "#     np.save(file, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare deepwalk based similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_name = 'Citeseer'\n",
    "test_ratio = 0.3\n",
    "rounds = 10\n",
    "window = 10\n",
    "b = 1\n",
    "dir_preprocessed = 'LP_data/' + graph_name + '_' + str(test_ratio) + '/'\n",
    "for round_i in range(rounds):\n",
    "    print(round_i)\n",
    "    f_train_graph  = dir_preprocessed + 'train_graph_'  + str(round_i) + '.gpickle'\n",
    "    train_graph  = func.load_list(f_train_graph)\n",
    "    A = nx.to_scipy_sparse_matrix(train_graph, weight='weight')\n",
    "    S = netmf.comp_S_DW(A, window, b) # numpy matrix\n",
    "    file = 'mats/DW_lp_'+str(test_ratio)+'/'+graph_name+'_T'+str(window)+'_b'+str(b)+'_'+str(round_i)+'.npy'\n",
    "    np.save(file, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare FE based similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_name = 'BlogCatalog'\n",
    "# test_ratio = 0.2\n",
    "# rounds = 10\n",
    "# inf = 1e8\n",
    "# beta_list = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1]\n",
    "# dir_preprocessed = 'LP_data/' + graph_name + '_' + str(test_ratio) + '/'\n",
    "\n",
    "# for round_i in range(rounds):\n",
    "#     print(round_i)\n",
    "#     f_train_graph  = dir_preprocessed + 'train_graph_'  + str(round_i) + '.gpickle'\n",
    "#     nx_G  = func.load_list(f_train_graph)\n",
    "#     A = np.array(nx.to_numpy_matrix(nx_G, weight='weight'))\n",
    "#     d = np.sum(A, 0)\n",
    "#     C = dissim.adj2cost(A, inf)\n",
    "#     graph_size = len(nx_G)\n",
    "#     H = np.eye(graph_size) - np.ones((graph_size, graph_size))/graph_size\n",
    "    \n",
    "#     for beta in beta_list:\n",
    "#         print(beta)\n",
    "#         Delta_FE = dissim.FE(A, C, d, beta)\n",
    "#         K = -0.5*np.matmul(np.matmul(H,Delta_FE),H) \n",
    "#         file = 'mats/FE_lp_'+str(test_ratio)+'/'+graph_name+'_'+str(beta)+'_'+str(round_i)+'.npy'\n",
    "#         np.save(file, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_name = 'BlogCatalog'\n",
    "test_ratio = 0.3\n",
    "rounds = 10\n",
    "inf = 1e8\n",
    "beta_list = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1]\n",
    "dir_preprocessed = 'LP_data/' + graph_name + '_' + str(test_ratio) + '/'\n",
    "\n",
    "for round_i in range(rounds):\n",
    "    print(round_i)\n",
    "    f_train_graph  = dir_preprocessed + 'train_graph_'  + str(round_i) + '.gpickle'\n",
    "    nx_G  = func.load_list(f_train_graph)\n",
    "    A = np.array(nx.to_numpy_matrix(nx_G, weight='weight'))\n",
    "    d = np.sum(A, 0)\n",
    "    C = dissim.adj2cost(A, inf)\n",
    "    \n",
    "    for beta in beta_list:\n",
    "        print(beta)\n",
    "        Delta_FE = dissim.FE(A, C, d, beta)\n",
    "        file = 'mats_v2/FE_lp_'+str(test_ratio)+'/'+graph_name+'_'+str(beta)+'_'+str(round_i)+'.npy'\n",
    "        np.save(file, Delta_FE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
